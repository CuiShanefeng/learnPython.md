

```python
import numpy as np
import matplotlib.pyplot as plt
import h5py
from lr_utils import load_dataset
```

    C:\Users\Surface\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      from ._conv import register_converters as _register_converters
    


```python
train_set_x_orig , train_set_y , test_set_x_orig , test_set_y , classes = load_dataset()
index = 24
plt.imshow(train_set_x_orig[index])
train_set_x_orig.shape
```




    (209, 64, 64, 3)




![png](output_1_1.png)



```python
m_train = train_set_y.shape[1]
m_test = test_set_y.shape[1]
num_px = train_set_x_orig.shape[1]
```

    209
    50
    


```python
train_set_x_flatten = train_set_x_orig.reshape( train_set_x_orig.shape[0],-1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T
```


```python
train_set_x = train_set_x_flatten / 255
test_set_x = test_set_x_flatten / 255
```


```python
def sigmoid(z):
    s = 1 / (1 + np.exp(-z))
    return s
```


```python
def initialize_with_zero_(dim):
    w = np.zeros(shape = (dim, 1))
    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))
```


```python
def propagate(w, b, X, Y):
    m = X.shape[1]
    A = sigmoid(np.dot(w.T, X) + b)
    cost = (-1 / m) * np.sum(Y * np.log(A) + (1-Y) * (np.log(1 - A)))
    dw = (1 / m) * np.dot(X, (A - Y).T)
    db = (1 / m) * np.sum(A - Y)
    
    assert(dw.shape == w.shape)
    assert(db.dtype == float)
    cost = np.squeeze(cost)
    assert(cost.shape == ())
    grads = {
                "dw": dw,
                "db": db
            }
    return (grads , cost)
```


```python
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = Falase)
    cost = []

    for i in range():
        grads, cost = propagate(w, b, X, Y)
        
        dw = grads["dw"]
        db = grads["db"]
        
        w = w - learning_rate * dw
        b = b - learning_rate * db
        if i % 100 == 0:
            costs.append(cost)
        if (print_cost) and (i % 100 == 0):
             print("迭代的次数: %i ， 误差值： %f" % (i,cost))
        params  = {
                "w" : w,
                "b" : b }
        grads = {
            "dw": dw,
            "db": db } 
        return (params , grads , costs)
```


```python
a = np.array([[1, 2, 3], [1, 2, 3],[1, 2, 3]])
b = np.array([1, 2, 3])

print(np.dot(a.T, b))
print(np.dot(a, b.T))
print(np.dot(a, b))
print(np.dot(b, a))
```

    [ 6 12 18]
    [14 14 14]
    [14 14 14]
    [ 6 12 18]
    
